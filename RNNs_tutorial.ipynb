{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Recurrent Neural Networks in Keras \n",
    "## CS175 Discussion #4,  Jan. 31st, 2018\n",
    "Author: [Eric Nalisnick](http://www.ics.uci.edu/~enalisni/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of this Lesson\n",
    "- Introduce\n",
    "    - RNNs for classification\n",
    "    - RNNs for language generation\n",
    "- Implement... \n",
    "    - Elman RNN\n",
    "    - Long Short-Term Memory (LSTM) RNN\n",
    "    - Convolutional RNNs\n",
    "\n",
    "### References \n",
    "- [Keras](https://keras.io/) \n",
    "- [*The Unreasonable Effectiveness of Recurrent Neural Networks* by Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- [*Understanding LSTM Networks* by Chris Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "## 0.  Python Preliminaries\n",
    "As usual, first we need to import Numpy and MatPlotLib..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  IMDB Dataset\n",
    "\n",
    "Let's start by loading the data we'll be working with.  We'll use the [IMDB dataset](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification), which contains 25,000 movie reviews labeled with their sentiment (positive vs negative)...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# truncation variables\n",
    "n_words_to_keep = 5000\n",
    "max_review_length = 500\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=n_words_to_keep)\n",
    "\n",
    "# pad / crop reviews so all are 300 'words'\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "[[   0    0    0 ...,   19  178   32]\n",
      " [   0    0    0 ...,   16  145   95]\n",
      " [   0    0    0 ...,    7  129  113]\n",
      " ..., \n",
      " [   0    0    0 ...,    4 3586    2]\n",
      " [   0    0    0 ...,   12    9   23]\n",
      " [   0    0    0 ...,  204  131    9]]\n",
      "\n",
      "Labels:\n",
      "[1 0 0 ..., 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\")\n",
    "print(X_train)\n",
    "\n",
    "print(\"\\nLabels:\")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Simple (Elman) RNN for Sentence Classification\n",
    "\n",
    "First, we define the most basic RNN implementation, also known as an Elman RNN, for binary classification.  Recall, that given the sequence of words in a review, we classify the review as positive ($y_{i}=1$) or negative ($y_{i}=0$).  First, let's write down the model's recurrent dynamics: $$ \\mathbf{h}_{i, t} = f_{h}( x_{i,t} \\mathbf{W} + \\mathbf{h}_{i,t-1} \\mathbf{U} + \\mathbf{b}_{1} )$$ where $f_{h}$ is some type of activation function (Keras defaults to *tanh*), $x_{i,t}$ is the input for the $i$th example at time step $t$, $\\mathbf{h}_{i,t-1}$ are the hidden units produced at the previous time step, and $\\mathbf{b}_{1}$ is a bias vector.  After all words in the review have been input into the RNN, it's time to generate a prediction: $$\\mathbb{E}[y_{i}] = f_{out}(\\mathbf{h}_{i, T} \\boldsymbol{\\beta} + \\mathbf{b}_{2}) $$ where $f_{out}$ is the logistic function in the case of binary classification.  Note that the hidden units are those computed at the *last* time step $t=T$.  To train the model, we again use the *cross-entropy* error function: $$ \\mathcal{L}_{CE} = -y_{i} \\log \\mathbb{E}[y_{i}] + -(1-y_{i}) \\log (1-\\mathbb{E}[y_{i}]).$$  Learning, i.e. model optimization, is performed by taking gradients w.r.t. $\\mathbf{W}$, $\\mathbf{U}$, $\\mathbf{b_{1}}$, $\\mathbf{b_{2}}$, and $\\boldsymbol{\\beta}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, SimpleRNN\n",
    "\n",
    "h1_size = 32\n",
    "h_RNN_size = 100\n",
    "output_size = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_words_to_keep, h1_size, input_length=max_review_length))\n",
    "model.add(SimpleRNN(units=h_RNN_size))\n",
    "model.add(Dense(output_size, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a loss function and an optimizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has a nice function for summarizing the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_7 (SimpleRNN)     (None, 100)               13300     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 173,401\n",
      "Trainable params: 173,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now train the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 187s 7ms/step - loss: 0.6561 - acc: 0.5888 - val_loss: 0.6569 - val_acc: 0.6092\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 195s 8ms/step - loss: 0.5788 - acc: 0.6926 - val_loss: 0.5967 - val_acc: 0.6685\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 195s 8ms/step - loss: 0.4618 - acc: 0.7840 - val_loss: 0.5093 - val_acc: 0.7453\n"
     ]
    }
   ],
   "source": [
    "train_log = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's get the final test error..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 59s 2ms/step\n",
      "Elman RNN Test Accuracy: 74.532%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Elman RNN Test Accuracy: %.3f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Long Short-Term Memory Network for Sentence Classification\n",
    "\n",
    "Elman networks have difficulties learning long-term dependencies in the input.  Recognizing this, [Hochreiter and Schmidhuber (1997)](http://www.bioinf.jku.at/publications/older/2604.pdf) proposed the *long short-term memory* unit to solve the problem.  We won't go into the details behind the unit; see [Chris Olah's blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) for an intuitive overview.  Running an LSTM in Keras is just a straightforward change of the code above..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_words_to_keep, h1_size, input_length=max_review_length))\n",
    "model.add(LSTM(units=h_RNN_size))\n",
    "model.add(Dense(output_size, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a loss function and an optimizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And summarize the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 832s 33ms/step - loss: 0.4475 - acc: 0.7823 - val_loss: 0.3501 - val_acc: 0.8583\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 760s 30ms/step - loss: 0.3145 - acc: 0.8715 - val_loss: 0.3146 - val_acc: 0.8687\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 714s 29ms/step - loss: 0.2700 - acc: 0.8927 - val_loss: 0.3134 - val_acc: 0.8703\n"
     ]
    }
   ],
   "source": [
    "train_log = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 145s 6ms/step\n",
      "LSTM RNN Test Accuracy: 87.032%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"LSTM RNN Test Accuracy: %.3f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Convolutional LSTM Networks for Sentence Classification\n",
    "\n",
    "As we know from the success of the [bag-of-words (BOW) assumption](https://en.wikipedia.org/wiki/Bag-of-words_model), classifying text doesn't necessarily need to account for word ordering.  In fact, LSTMs can fail to generalize if they become too particular to word orderings in the training data.  One way to bridge the BOW assumption and the strict order dependence of RNNs is to add a one-dimensional [convolutional layer](https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layer) before the recurrent units.  Intuitively, this means that the RNN's input is order invariant outside of some filter size.  For instance, the network would treat the sentences 'the cat ran around the house' and 'around the house, the cat ran' roughly the same.  \n",
    "\n",
    "Let's define the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_words_to_keep, h1_size, input_length=max_review_length))\n",
    "\n",
    "# add convolutional layer w/ max pooling\n",
    "n_filters = 32 # number of output features\n",
    "filter_size = 3 # length of window\n",
    "model.add(Conv1D(filters=n_filters, kernel_size=filter_size, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(LSTM(units=h_RNN_size))\n",
    "model.add(Dense(output_size, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a loss function and an optimizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "10112/25000 [===========>..................] - ETA: 4:01 - loss: 0.6034 - acc: 0.6570"
     ]
    }
   ],
   "source": [
    "train_log = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"CNN-LSTM RNN Test Accuracy: %.3f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  LSTM Network for Language Generation\n",
    "\n",
    "RNNs are unique from the classification models we've been studying so far in that they are *generative*.  In other words, we can train them to generate language, not just classify it.  Yet, training a generative model essentially reverts to classification: the model predicts what will come next.  They do this by factorizing the joint probability as $p(w_{1},\\ldots,w_{T}) = p(w_{1})p(w_{2}|w_{1})p(w_{3}|w_{2},w_{1})\\ldots p(w_{T}|w_{T-1},\\ldots,w_{1})$.  Writing these probablities as a maximum likelihood objective, we have: $$ \\mathcal{L}_{gen} = -\\sum_{t=1}^{T} \\log p(w_{t} | w_{t-1},\\ldots,w_{1}).$$  \n",
    "\n",
    "Let's build a language model for Herman Melville's *Moby Dick*.  First, we read the text file and tokenize to extract the most common 10,000 words... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "with open('data/moby_dick.txt', 'r') as f:\n",
    "    moby_dick_text = f.read()\n",
    "    \n",
    "# translate text into int tokens\n",
    "vocab_size = 10000 + 1\n",
    "tokenizer = Tokenizer(num_words=vocab_size-1)\n",
    "tokenizer.fit_on_texts([moby_dick_text])\n",
    "encoded_text = tokenizer.texts_to_sequences([moby_dick_text])[0]\n",
    "\n",
    "print(\"TEXT: \"+moby_dick_text[69:300])\n",
    "print(\"\\nENCODINGS: \"+str(encoded_text[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we form sequences... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word -> word sequences\n",
    "sequences = list()\n",
    "for i in range(1, len(encoded_text)):\n",
    "    sequence = encoded_text[i-1:i+1]\n",
    "    sequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, make training labels out of the subsequent words... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into X and y elements\n",
    "sequences = np.array(sequences)\n",
    "X_train, y_train = sequences[:,0], sequences[:,1]\n",
    "y_train = to_categorical(y_train, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define an LSTM just as before (but a bit bigger one, this time)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_size = 100\n",
    "h_RNN_size = 500\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, h1_size, input_length=1))\n",
    "model.add(LSTM(h_RNN_size))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer and train..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.005)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_log = model.fit(X_train, y_train, epochs=10, batch_size=64)\n",
    "model = load_model('moby_dick_rnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  Sampling from the Model\n",
    "\n",
    "After training, we can simulate 'new' Moby Dick passages by sampling from the model.  Given a seed word, we run a forward pass through the model and sample from the resutling softmax over words: $$ w_{t+1} \\sim p(w_{t+1} | w_{t},\\ldots,w_{1}) = \\text{Multinoulli}(p=f_{softmax}(w_{1},\\ldots,w_{t})).$$  We then use the sampled word as input to the model for the next time step.  By repeating this process, we can then generate whole passages.  Here's a function that performs the sampling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: there are more effecient way to implement this\n",
    "def generate_seq(model, tokenizer, seed_text, n_words, temperature=1.):\n",
    "    in_text, result = seed_text, seed_text\n",
    "    \n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        encoded = np.array(encoded)\n",
    "        \n",
    "        # get the probabilities of each vocabulary word\n",
    "        word_probs = model.predict(encoded, verbose=0)[0]\n",
    "        \n",
    "        # add a temperature to create more sampling variation\n",
    "        word_probs = np.log(word_probs) / temperature \n",
    "        word_probs = np.exp(word_probs) / np.sum(np.exp(word_probs)) \n",
    "        yhat = np.random.choice(range(word_probs.shape[0]), p=word_probs)\n",
    "        \n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "                \n",
    "        # append to input\n",
    "        in_text, result = out_word, result + ' ' + out_word\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_seq(model, tokenizer, seed_text='ship', n_words=50, temperature=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_seq(model, tokenizer, seed_text='ship', n_words=50, temperature=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_seq(model, tokenizer, seed_text='ship', n_words=50, temperature=.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
