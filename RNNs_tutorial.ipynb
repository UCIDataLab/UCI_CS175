{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Recurrent Neural Networks in Keras \n",
    "## CS175 Discussion #4,  Jan. 31st, 2018\n",
    "Author: [Eric Nalisnick](http://www.ics.uci.edu/~enalisni/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of this Lesson\n",
    "- Introduce\n",
    "    - RNNs for classification\n",
    "    - RNNs for language generation\n",
    "- Implement... \n",
    "    - Elman RNN\n",
    "    - Long Short-Term Memory (LSTM) RNN\n",
    "    - Convolutional RNNs\n",
    "\n",
    "### References \n",
    "- [Keras](https://keras.io/) \n",
    "- [*The Unreasonable Effectiveness of Recurrent Neural Networks* by Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- [*Understanding LSTM Networks* by Chris Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "## 0.  Python Preliminaries\n",
    "As usual, first we need to import Numpy and MatPlotLib..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  IMDB Dataset\n",
    "\n",
    "Let's start by loading the data we'll be working with.  We'll use the [IMDB dataset](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification), which contains 25,000 movie reviews labeled with their sentiment (positive vs negative)...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# truncation variables\n",
    "n_words_to_keep = 5000\n",
    "max_review_length = 500\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=n_words_to_keep)\n",
    "\n",
    "# pad / crop reviews so all are 300 'words'\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "[[   0    0    0 ...,   19  178   32]\n",
      " [   0    0    0 ...,   16  145   95]\n",
      " [   0    0    0 ...,    7  129  113]\n",
      " ..., \n",
      " [   0    0    0 ...,    4 3586    2]\n",
      " [   0    0    0 ...,   12    9   23]\n",
      " [   0    0    0 ...,  204  131    9]]\n",
      "\n",
      "Labels:\n",
      "[1 0 0 ..., 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\")\n",
    "print(X_train)\n",
    "\n",
    "print(\"\\nLabels:\")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Simple (Elman) RNN for Sentence Classification\n",
    "\n",
    "First, we define the most basic RNN implementation, also known as an Elman RNN, for binary classification.  Recall, that given the sequence of words in a review, we classify the review as positive ($y_{i}=1$) or negative ($y_{i}=0$).  First, let's write down the model's recurrent dynamics: $$ \\mathbf{h}_{i, t} = f_{h}( x_{i,t} \\mathbf{W} + \\mathbf{h}_{i,t-1} \\mathbf{U} + \\mathbf{b}_{1} )$$ where $f_{h}$ is some type of activation function (Keras defaults to *tanh*), $x_{i,t}$ is the input for the $i$th example at time step $t$, $\\mathbf{h}_{i,t-1}$ are the hidden units produced at the previous time step, and $\\mathbf{b}_{1}$ is a bias vector.  After all words in the review have been input into the RNN, it's time to generate a prediction: $$\\mathbb{E}[y_{i}] = f_{out}(\\mathbf{h}_{i, T} \\boldsymbol{\\beta} + \\mathbf{b}_{2}) $$ where $f_{out}$ is the logistic function in the case of binary classification.  Note that the hidden units are those computed at the *last* time step $t=T$.  To train the model, we again use the *cross-entropy* error function: $$ \\mathcal{L}_{CE} = -y_{i} \\log \\mathbb{E}[y_{i}] + -(1-y_{i}) \\log (1-\\mathbb{E}[y_{i}]).$$  Learning, i.e. model optimization, is performed by taking gradients w.r.t. $\\mathbf{W}$, $\\mathbf{U}$, $\\mathbf{b_{1}}$, $\\mathbf{b_{2}}$, and $\\boldsymbol{\\beta}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, SimpleRNN\n",
    "\n",
    "h1_size = 32\n",
    "h_RNN_size = 100\n",
    "output_size = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_words_to_keep, h1_size, input_length=max_review_length))\n",
    "model.add(SimpleRNN(units=h_RNN_size))\n",
    "model.add(Dense(output_size, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a loss function and an optimizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has a nice function for summarizing the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_6 (SimpleRNN)     (None, 100)               13300     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 173,401\n",
      "Trainable params: 173,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now train the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 186s 7ms/step - loss: 0.6781 - acc: 0.5650 - val_loss: 0.6462 - val_acc: 0.6092\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 179s 7ms/step - loss: 0.5753 - acc: 0.7163 - val_loss: 0.5979 - val_acc: 0.6818\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 179s 7ms/step - loss: 0.5066 - acc: 0.7574 - val_loss: 0.5162 - val_acc: 0.7622\n"
     ]
    }
   ],
   "source": [
    "train_log = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's get the final test error..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 53s 2ms/step\n",
      "Elman RNN Test Accuracy: 76.224%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Elman RNN Test Accuracy: %.3f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Long Short-Term Memory Network for Sentence Classification\n",
    "\n",
    "Elman networks have difficulties learning long-term dependencies in the input.  Recognizing this, [Hochreiter and Schmidhuber (1997)](http://www.bioinf.jku.at/publications/older/2604.pdf) proposed the *long short-term memory* unit to solve the problem.  We won't go into the details behind the unit; see [Chris Olah's blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) for an intuitive overview.  Running an LSTM in Keras is just a straightforward change of the code above..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_words_to_keep, h1_size, input_length=max_review_length))\n",
    "model.add(LSTM(units=h_RNN_size))\n",
    "model.add(Dense(output_size, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a loss function and an optimizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And summarize the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 800s 32ms/step - loss: 0.4909 - acc: 0.7532 - val_loss: 0.3452 - val_acc: 0.8561\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 940s 38ms/step - loss: 0.3106 - acc: 0.8741 - val_loss: 0.3358 - val_acc: 0.8596\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 2328s 93ms/step - loss: 0.2658 - acc: 0.8922 - val_loss: 0.3198 - val_acc: 0.8699\n"
     ]
    }
   ],
   "source": [
    "train_log = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 180s 7ms/step\n",
      "LSTM RNN Test Accuracy: 86.988%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"LSTM RNN Test Accuracy: %.3f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Convolutional LSTM Networks for Sentence Classification\n",
    "\n",
    "As we know from the success of the [bag-of-words (BOW) assumption](https://en.wikipedia.org/wiki/Bag-of-words_model), classifying text doesn't necessarily need to account for word ordering.  In fact, LSTMs can fail to generalize if they become too particular to word orderings in the training data.  One way to bridge the BOW assumption and the strict order dependence of RNNs is to add a one-dimensional [convolutional layer](https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layer) before the recurrent units.  Intuitively, this means that the RNN's input is order invariant outside of some filter size.  For instance, the network would treat the sentences 'the cat ran around the house' and 'around the house, the cat ran' roughly the same.  \n",
    "\n",
    "Let's define the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_words_to_keep, h1_size, input_length=max_review_length))\n",
    "\n",
    "# add convolutional layer w/ max pooling\n",
    "n_filters = 32 # number of output features\n",
    "filter_size = 3 # length of window\n",
    "model.add(Conv1D(filters=n_filters, kernel_size=filter_size, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(LSTM(units=h_RNN_size))\n",
    "model.add(Dense(output_size, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a loss function and an optimizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 602s 24ms/step - loss: 0.4120 - acc: 0.8043 - val_loss: 0.2760 - val_acc: 0.8889\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 483s 19ms/step - loss: 0.2429 - acc: 0.9043 - val_loss: 0.2851 - val_acc: 0.8831\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 506s 20ms/step - loss: 0.1981 - acc: 0.9254 - val_loss: 0.2942 - val_acc: 0.8860\n"
     ]
    }
   ],
   "source": [
    "train_log = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 107s 4ms/step\n",
      "CNN-LSTM RNN Test Accuracy: 88.604%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"CNN-LSTM RNN Test Accuracy: %.3f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  LSTM Network for Language Generation\n",
    "\n",
    "RNNs are unique from the classification models we've been studying so far in that they are *generative*.  In other words, we can train them to generate language, not just classify it.  Yet, we do this generative modeling by turning the problem into one of classification.  Given some observed words, we train the network to predict the next one, i.e. $$ \\mathcal{L}_{gen} = -\\sum_{t=1}^{T} \\log p(w_{t} | w_{t-1},\\ldots,w_{1}).$$  \n",
    "\n",
    "Let's build a language model for Herman Melville's *Moby Dick*... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: Call me Ishmael.  Some years ago--never mind how long\n",
      "precisely--having little or no money in my purse, and nothing\n",
      "particular to interest me on shore, I thought I would sail about a\n",
      "little and see the watery part of the world.  It\n",
      "\n",
      "ENCODINGS: [272, 280, 35, 1, 28, 21, 6732, 6733, 154, 6734, 9631, 399, 39, 1060, 42, 286, 608, 135, 287, 107, 79, 764, 349, 108, 35, 48, 1807, 6, 46, 2700, 3, 214, 437, 5, 1192, 39, 26, 934, 10, 173, 10, 64, 232, 84, 4, 108, 3, 104, 1, 828, 174, 2, 1, 167, 9, 15, 4, 102, 10, 34, 2, 2162, 126, 1, 9632, 3, 9633, 1, 9634, 1339, 10, 400, 341, 2163, 1808, 84, 1, 387, 1339, 9, 15, 4, 2164, 9635, 6735, 6, 46, 288, 1339, 10, 400, 341, 1645, 1977, 91, 429, 6736, 3, 1978, 53]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "with open('data/moby_dick.txt', 'r') as f:\n",
    "    moby_dick_text = f.read()\n",
    "    \n",
    "# translate text into int tokens\n",
    "vocab_size = 10000 + 1\n",
    "tokenizer = Tokenizer(num_words=vocab_size-1)\n",
    "tokenizer.fit_on_texts([moby_dick_text])\n",
    "encoded_text = tokenizer.texts_to_sequences([moby_dick_text])[0]\n",
    "\n",
    "print(\"TEXT: \"+moby_dick_text[69:300])\n",
    "print(\"\\nENCODINGS: \"+str(encoded_text[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 205292\n"
     ]
    }
   ],
   "source": [
    "# create word -> word sequences\n",
    "sequences = list()\n",
    "for i in range(1, len(encoded_text)):\n",
    "    sequence = encoded_text[i-1:i+1]\n",
    "    sequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into X and y elements\n",
    "sequences = np.array(sequences)\n",
    "X_train, y_train = sequences[:,0], sequences[:,1]\n",
    "y_train = to_categorical(y_train, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 1, 32)             320032    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10001)             1010101   \n",
      "=================================================================\n",
      "Total params: 1,383,333\n",
      "Trainable params: 1,383,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, h1_size, input_length=1))\n",
    "model.add(LSTM(h_RNN_size))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.005)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 48576/205292 [======>.......................] - ETA: 4:26 - loss: 6.4958 - acc: 0.0928"
     ]
    }
   ],
   "source": [
    "train_log = model.fit(X_train, y_train, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, tokenizer, seed_text, n_words):\n",
    "    in_text, result = seed_text, seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        encoded = np.array(encoded)\n",
    "        # predict a word in the vocabulary\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text, result = out_word, result + ' ' + out_word\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_seq(model, tokenizer, 'ship', 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
